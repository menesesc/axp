# âœ… Setup Checklist - Worker AXP

## 1ï¸âƒ£ Base de Datos (PostgreSQL + Prisma)

### OpciÃ³n A: Usando Supabase (Recomendado)

```bash
# Ya tenÃ©s PostgreSQL, solo necesitÃ¡s aplicar el schema

# 1. Obtener connection string de Supabase
# Dashboard â†’ Settings â†’ Database â†’ Connection string
# Ejemplo: postgresql://postgres:[PASSWORD]@db.[PROJECT].supabase.co:5432/postgres

# 2. Configurar Prisma
cd packages/database
echo "DATABASE_URL=\"postgresql://postgres:[PASSWORD]@db.[PROJECT].supabase.co:5432/postgres\"" > .env

# 3. Aplicar schema (crear tablas)
bun run prisma:push

# 4. Verificar
bun run prisma studio
# DeberÃ­a abrir browser con todas las tablas
```

### OpciÃ³n B: PostgreSQL Local

```bash
# 1. Instalar PostgreSQL
brew install postgresql@15
brew services start postgresql@15

# 2. Crear base de datos
createdb axp

# 3. Configurar Prisma
cd packages/database
echo "DATABASE_URL=\"postgresql://localhost:5432/axp?schema=public\"" > .env

# 4. Aplicar schema
bun run prisma:push

# 5. Verificar
psql axp -c "\dt"  # Lista todas las tablas
# DeberÃ­as ver: Cliente, Usuario, Proveedor, Documento, etc.
```

### VerificaciÃ³n

```bash
# DeberÃ­as ver estas tablas:
âœ… Cliente
âœ… Usuario
âœ… Proveedor
âœ… Documento
âœ… DocumentoItem
âœ… DocumentoRevision
âœ… Pago
âœ… PagoMetodo
âœ… PagoDocumento
âœ… IngestQueue
```

---

## 2ï¸âƒ£ Cloudflare R2 (Almacenamiento de PDFs)

### Paso 1: Crear Bucket

1. Ir a https://dash.cloudflare.com/
2. Click **R2** en menÃº lateral
3. Click **"Create bucket"**
4. Nombre: `axp-documents`
5. Click **"Create bucket"**

### Paso 2: Obtener Account ID

- MirÃ¡ la URL del dashboard:
  ```
  https://dash.cloudflare.com/[ESTE_ES_TU_ACCOUNT_ID]/r2/overview
  ```
- O en **R2 Overview** â†’ "Account ID" (esquina superior derecha)

### Paso 3: Crear API Token

1. En R2, click **"Manage R2 API Tokens"**
2. Click **"Create API Token"**
3. ConfiguraciÃ³n:
   - Name: `axp-worker-access`
   - Permissions: **Object Read & Write**
   - Bucket scope: **Apply to specific buckets** â†’ `axp-documents`
   - TTL: **Never expire**
4. Click **"Create API Token"**
5. **âš ï¸ COPIAR INMEDIATAMENTE (no se puede ver despuÃ©s):**
   ```
   Access Key ID: abc123...
   Secret Access Key: xyz789...
   ```

### VerificaciÃ³n

```bash
# Test con AWS CLI (R2 es compatible S3)
export AWS_ACCESS_KEY_ID="[TU_R2_ACCESS_KEY]"
export AWS_SECRET_ACCESS_KEY="[TU_R2_SECRET_KEY]"
export AWS_ENDPOINT_URL="https://[ACCOUNT_ID].r2.cloudflarestorage.com"

aws s3 ls --endpoint-url=$AWS_ENDPOINT_URL
# DeberÃ­as ver: axp-documents
```

---

## 3ï¸âƒ£ Crear Cliente de Prueba en Base de Datos

```bash
# Conectar a tu base de datos
psql $DATABASE_URL

# Insertar cliente de prueba
INSERT INTO "Cliente" (
  id, 
  nombre, 
  cuit, 
  "razonSocial", 
  email, 
  telefono, 
  direccion, 
  ciudad, 
  provincia, 
  "codigoPostal", 
  pais, 
  activo
) VALUES (
  '00000000-0000-0000-0000-000000000001',
  'Weiss Cliente Test',
  '33712152449',
  'Weiss S.A.',
  'contacto@weiss.com',
  '1145678900',
  'Av. Corrientes 1234',
  'Buenos Aires',
  'CABA',
  'C1043',
  'Argentina',
  true
);

# Verificar
SELECT id, nombre, cuit FROM "Cliente";
```

**Guardar el UUID del cliente** (lo necesitÃ¡s para prefix-map.json)

---

## 4ï¸âƒ£ Configurar Worker

### Archivo .env

```bash
cd apps/worker
cp .env.example .env
vim .env
```

**Completar con tus valores:**
```bash
# Database (mismo que usaste en packages/database)
DATABASE_URL="postgresql://..."

# Cloudflare R2 (del paso 2)
R2_ACCOUNT_ID="[TU_ACCOUNT_ID]"
R2_ACCESS_KEY_ID="[TU_ACCESS_KEY]"
R2_SECRET_ACCESS_KEY="[TU_SECRET_KEY]"
R2_BUCKET_NAME="axp-documents"

# Worker Mode
WORKER_MODE="watcher"

# Directories (crear despuÃ©s)
WEBDAV_DIR="/tmp/axp-test/data"
PROCESSED_DIR="/tmp/axp-test/processed"
FAILED_DIR="/tmp/axp-test/failed"

# Intervals
WATCHER_POLL_INTERVAL="2000"
FILE_STABLE_CHECKS="3"
MAX_CONCURRENT_JOBS="5"
PROCESSOR_POLL_INTERVAL="5000"
MAX_RETRY_ATTEMPTS="5"

# Prefix Map
PREFIX_MAP_PATH="./prefix-map.json"
```

### Archivo prefix-map.json

```bash
cd apps/worker
cp prefix-map.example.json prefix-map.json
vim prefix-map.json
```

**Completar con el UUID del cliente:**
```json
{
  "weiss": {
    "clienteId": "00000000-0000-0000-0000-000000000001",
    "cuit": "33712152449",
    "r2Prefix": "cuit=33712152449"
  }
}
```

**âš ï¸ El `clienteId` debe coincidir con el UUID del cliente en la base de datos**

---

## 5ï¸âƒ£ Crear Directorios WebDAV

```bash
# Para testing local
mkdir -p /tmp/axp-test/data
mkdir -p /tmp/axp-test/processed
mkdir -p /tmp/axp-test/failed

# Para producciÃ³n (en servidor)
sudo mkdir -p /srv/webdav/data
sudo mkdir -p /srv/webdav/processed
sudo mkdir -p /srv/webdav/failed
sudo chown -R $USER:$USER /srv/webdav
```

---

## 6ï¸âƒ£ Test RÃ¡pido

### Terminal 1: Watcher
```bash
cd apps/worker
bun run dev:watcher
```

### Terminal 2: Crear archivo de prueba
```bash
echo "PDF de prueba" > /tmp/axp-test/data/weiss_test.pdf
```

### Logs esperados en Terminal 1:
```
[WATCHER] ğŸ“„ Found new file: weiss_test.pdf
[WATCHER] â³ Waiting for file to be stable: weiss_test.pdf
[WATCHER] ğŸ¢ Detected prefix: weiss
[WATCHER] âœ… Cliente: 33712152449 (00000000-0000-0000-0000-000000000001)
[WATCHER] ğŸ” Calculating SHA256...
[WATCHER] ğŸ” SHA256: abc123...
[WATCHER] ğŸ“ Enqueuing file for processing...
[WATCHER] âœ… File enqueued: weiss_test.pdf (queue id: xyz)
[WATCHER] ğŸ“¦ File moved to processed: /tmp/axp-test/processed/weiss_test.pdf
```

### Terminal 3: Processor
```bash
cd apps/worker
bun run dev:processor
```

### Logs esperados en Terminal 3:
```
[PROCESSOR] ğŸš€ Queue Processor starting...
[PROCESSOR] ğŸ“‹ Found 1 pending item(s)
[PROCESSOR] ğŸ”„ Processing queue item: xyz (weiss_test.pdf)
[PROCESSOR] ğŸ“– Reading file: /tmp/axp-test/processed/weiss_test.pdf
[PROCESSOR] ğŸ¢ Cliente: 33712152449
[PROCESSOR] ğŸ”‘ R2 key: cuit=33712152449/2026/01/03/weiss_test.pdf
[PROCESSOR] â˜ï¸  Uploading to R2...
[PROCESSOR] âœ… Upload successful: cuit=33712152449/2026/01/03/weiss_test.pdf (234ms)
[PROCESSOR] âœ… Queue item processed successfully: xyz
```

### Verificar en R2:
1. Ir a Cloudflare Dashboard â†’ R2 â†’ axp-documents
2. Navegar a: `cuit=33712152449/2026/01/03/`
3. DeberÃ­as ver: `weiss_test.pdf`

---

## 7ï¸âƒ£ Troubleshooting

### Error: "Cannot find name 'process'"
- âœ… **Esto es normal** - Los errores de TypeScript son esperados
- El cÃ³digo funciona perfectamente con `bun run`
- IgnorÃ¡ los errores del editor

### Error: "Cannot connect to database"
```bash
# Verificar conexiÃ³n
psql $DATABASE_URL -c "SELECT 1"

# Si falla, revisar:
# 1. URL correcta
# 2. PostgreSQL corriendo (brew services list)
# 3. Firewall/VPN no bloquea
```

### Error: "No client configuration found for prefix"
```bash
# Verificar prefix-map.json
cat apps/worker/prefix-map.json

# Verificar que el clienteId existe en DB
psql $DATABASE_URL -c "SELECT id, nombre FROM \"Cliente\" WHERE id = '00000000-0000-0000-0000-000000000001';"
```

### Error: "R2 upload failed"
```bash
# Test credenciales
export AWS_ACCESS_KEY_ID="[R2_ACCESS_KEY]"
export AWS_SECRET_ACCESS_KEY="[R2_SECRET_KEY]"
aws s3 ls --endpoint-url=https://[ACCOUNT_ID].r2.cloudflarestorage.com

# Si falla:
# 1. Verificar Access Key ID y Secret
# 2. Verificar Account ID
# 3. Verificar permisos del token (Object Read & Write)
```

---

## ğŸ“‹ Checklist Final

Antes de ejecutar, asegurate que:

- [ ] PostgreSQL estÃ¡ corriendo
- [ ] Prisma schema aplicado (tablas creadas)
- [ ] Cliente de prueba insertado en DB
- [ ] Cloudflare R2 bucket creado
- [ ] API Token de R2 obtenido
- [ ] `.env` configurado con credenciales correctas
- [ ] `prefix-map.json` con clienteId correcto
- [ ] Directorios WebDAV creados
- [ ] Tests estructurales pasan: `bun run test-structure.ts`

---

## ğŸš€ Ejecutar en ProducciÃ³n

Una vez que todo funcione localmente:

```bash
# 1. Actualizar .env con paths de producciÃ³n
WEBDAV_DIR="/srv/webdav/data"
PROCESSED_DIR="/srv/webdav/processed"
FAILED_DIR="/srv/webdav/failed"

# 2. Copiar prefix-map.json a producciÃ³n
scp prefix-map.json server:/etc/axp/prefix-map.json

# 3. Build y deploy con Docker
docker-compose build
docker-compose up -d

# 4. Verificar logs
docker-compose logs -f
```

---

## ğŸ’¡ Tips

1. **Testear siempre con archivos pequeÃ±os primero** (< 1 MB)
2. **MonitoreÃ¡ los logs constantemente** durante las primeras subidas
3. **VerificÃ¡ en Prisma Studio** que los registros se crean en IngestQueue
4. **VerificÃ¡ en R2 Dashboard** que los archivos se suben correctamente
5. **EmpezÃ¡ con 1 archivo, luego escalÃ¡** a mÃºltiples archivos

---

Â¿Alguno de estos pasos no estÃ¡ claro? Â¡Preguntame!
